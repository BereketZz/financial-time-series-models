# -*- coding: utf-8 -*-
"""task2_forecasting.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15N7Wmo847pvzW4T9cJHPL71FwuZDp2Es
"""

# ============================
# Task 2: TSLA ARIMA vs LSTM
# ============================

# --- Installs (quiet) ---
!pip install -q yfinance pmdarima tensorflow==2.12.0

# --- Imports ---
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pmdarima as pm
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from math import sqrt

# Reproducibility
np.random.seed(42)
tf.random.set_seed(42)

plt.rcParams["figure.figsize"] = (12, 6)

# -----------------------------
# 1) Load TSLA data robustly
# -----------------------------
ticker = "TSLA"
start_date = "2015-07-01"
end_date   = "2025-07-31"

raw = yf.download(ticker, start=start_date, end=end_date, progress=False)

if raw.empty:
    raise SystemExit("No data returned. Check ticker or date range.")

# Prefer Adj Close, fallback to Close
price_col = "Adj Close" if "Adj Close" in raw.columns else "Close"
if price_col != "Adj Close":
    print("⚠️ 'Adj Close' missing; using 'Close' instead.")

ts = raw[[price_col]].copy()
ts.columns = ["AdjClose"]
ts.index = pd.to_datetime(ts.index)
ts = ts.sort_index().dropna()

print(f"Loaded {len(ts)} rows for {ticker}. Range: {ts.index.min().date()} → {ts.index.max().date()}")
display(ts.head())

# Quick sanity plot
ts["AdjClose"].plot(title=f"{ticker} {price_col} Price", ylabel="USD")
plt.show()

# -----------------------------
# 2) Chronological Split
# -----------------------------
train_end = "2023-12-31"
test_start = "2024-01-01"

train = ts.loc[:train_end].copy()
test  = ts.loc[test_start:].copy()

if len(test) == 0:
    raise SystemExit("Test set is empty. Adjust date boundaries.")

print(f"Train: {len(train)} rows | Test: {len(test)} rows")

# -----------------------------
# 3) ARIMA via auto_arima
#    (Model log prices for stability)
# -----------------------------
print("\n--- ARIMA: auto_arima (this may take a minute) ---")
train_log = np.log(train["AdjClose"].values)

arima_model = pm.auto_arima(
    train_log,
    seasonal=False,          # set True+m if you want SARIMA
    stepwise=True,
    suppress_warnings=True,
    error_action="ignore",
    max_p=5, max_q=5,
    trace=False
)
print("Chosen ARIMA order:", arima_model.order)

# Forecast for full test horizon
n_periods = len(test)
arima_forecast_log = arima_model.predict(n_periods=n_periods)
arima_forecast = np.exp(arima_forecast_log)  # invert log

arima_pred = pd.Series(arima_forecast, index=test.index, name="ARIMA_Pred")

# -----------------------------
# 4) LSTM
#    Scaling + Sliding Window
# -----------------------------
print("\n--- LSTM: training ---")
lookback = 60  # days of history to predict next day

scaler = MinMaxScaler(feature_range=(0, 1))
train_vals = train["AdjClose"].values.reshape(-1, 1)
test_vals  = test["AdjClose"].values.reshape(-1, 1)

scaler.fit(train_vals)
train_scaled = scaler.transform(train_vals)

# For generating sequences across the train/test boundary, concatenate
all_vals = np.vstack([train_vals, test_vals])
all_scaled = scaler.transform(all_vals)

def make_sequences(arr, start_idx, end_idx, window):
    X, y = [], []
    for i in range(start_idx + window, end_idx):
        X.append(arr[i - window:i, 0])
        y.append(arr[i, 0])
    return np.array(X), np.array(y)

# Train sequences: from 0..len(train)-1
X_train, y_train = make_sequences(all_scaled, 0, len(train_vals), lookback)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))

# Test sequences: start lookback steps before test starts
start_idx = len(train_vals) - lookback
end_idx   = len(train_vals) + len(test_vals)
X_test_seq, y_test_seq = make_sequences(all_scaled, start_idx, end_idx, lookback)
X_test = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))

# True y for test (original scale) aligned to sequences
y_test_true = test_vals[:len(y_test_seq)].reshape(-1)

# Build LSTM
tf.keras.backend.clear_session()
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(lookback, 1)),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1)
])
model.compile(optimizer="adam", loss="mse")
model.summary()

es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
history = model.fit(
    X_train, y_train,
    validation_split=0.1,
    epochs=100,
    batch_size=32,
    callbacks=[es],
    verbose=1
)

# Predict (inverse scale)
lstm_pred_scaled = model.predict(X_test)
lstm_pred = scaler.inverse_transform(lstm_pred_scaled).reshape(-1)
lstm_index = test.index[:len(lstm_pred)]
lstm_pred = pd.Series(lstm_pred, index=lstm_index, name="LSTM_Pred")

# -----------------------------
# 5) Metrics: MAE, RMSE, MAPE
# -----------------------------
def mape(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    return np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-8, y_true))) * 100

# ARIMA metrics
ar_true = test["AdjClose"].values[:len(arima_pred)]
ar_pred = arima_pred.values
mae_ar  = mean_absolute_error(ar_true, ar_pred)
rmse_ar = sqrt(mean_squared_error(ar_true, ar_pred))
mape_ar = mape(ar_true, ar_pred)

# LSTM metrics (align with lstm_pred length)
ls_true = test.loc[lstm_pred.index, "AdjClose"].values
ls_pred = lstm_pred.values
mae_ls  = mean_absolute_error(ls_true, ls_pred)
rmse_ls = sqrt(mean_squared_error(ls_true, ls_pred))
mape_ls = mape(ls_true, ls_pred)

print("\n=== Performance (Test) ===")
print(f"ARIMA -> MAE: {mae_ar:.2f} | RMSE: {rmse_ar:.2f} | MAPE: {mape_ar:.2f}%")
print(f"LSTM  -> MAE: {mae_ls:.2f} | RMSE: {rmse_ls:.2f} | MAPE: {mape_ls:.2f}%")

# -----------------------------
# 6) Plots: Actual vs Forecasts
# -----------------------------
plt.figure(figsize=(14,6))
plt.plot(test.index, test["AdjClose"], label="Actual", color="black")
plt.plot(arima_pred.index, arima_pred, label="ARIMA Forecast", alpha=0.8)
plt.plot(lstm_pred.index, lstm_pred, label="LSTM Forecast", alpha=0.8)
plt.title("TSLA — Actual vs Forecasts (Test Period)")
plt.xlabel("Date"); plt.ylabel("USD"); plt.legend()
plt.show()

# Zoom into last 120 test days (if available)
if len(test) > 120:
    zoom_idx = test.index[-120:]
    plt.figure(figsize=(14,6))
    plt.plot(test.loc[zoom_idx, "AdjClose"], label="Actual", color="black")
    if any(arima_pred.index.isin(zoom_idx)):
        plt.plot(arima_pred.loc[arima_pred.index.isin(zoom_idx)], label="ARIMA Forecast")
    if any(lstm_pred.index.isin(zoom_idx)):
        plt.plot(lstm_pred.loc[lstm_pred.index.isin(zoom_idx)], label="LSTM Forecast")
    plt.title("TSLA — Last 120 Test Days: Actual vs Forecasts")
    plt.xlabel("Date"); plt.ylabel("USD"); plt.legend()
    plt.show()

# -----------------------------
# 7) Brief discussion helper
# -----------------------------
print("\n--- Notes ---")
print("- ARIMA is simpler and more interpretable; great for short-term linear dynamics.")
print("- LSTM can capture nonlinear patterns but needs tuning and more compute.")
print("- Select the model with lower MAE/RMSE/MAPE, but weigh business needs (speed, explainability).")